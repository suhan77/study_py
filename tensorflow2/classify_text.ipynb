{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출처 : https://www.tensorflow.org/alpha/tutorials/keras/basic_text_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 노트북은 영화 리뷰(review) 텍스트를 긍정(positive) 또는 부정(negative)으로 분류합니다. 이 예제는 이진(binary)-또는 클래스(class)가 두 개인- 분류 문제입니다. 이진 분류는 머신러닝에서 중요하고 널리 사용됩니다.\n",
    "\n",
    "여기에서는 인터넷 영화 데이터베이스(Internet Movie Database)에서 수집한 50,000개의 영화 리뷰 텍스트를 담은 IMDB 데이터셋을 사용하겠습니다. 25,000개 리뷰는 훈련용으로, 25,000개는 테스트용으로 나뉘어져 있습니다. 훈련 세트와 테스트 세트의 클래스는 균형이 잡혀 있습니다. 즉 긍정적인 리뷰와 부정적인 리뷰의 개수가 동일합니다.\n",
    "\n",
    "이 노트북은 모델을 만들고 훈련하기 위해 텐서플로의 고수준 파이썬 API인 tf.keras를 사용합니다. tf.keras를 사용한 고급 텍스트 분류 튜토리얼은 MLCC 텍스트 분류 가이드를 참고하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-alpha0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 데이터 임포트 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "리뷰(단어의 시퀀스(sequence))는 미리 전처리해서 정수 시퀀스로 변환되어 있습니다. 각 정수는 어휘 사전에 있는 특정 단어를 의미합니다.\n",
    "\n",
    "매개변수 num_words=10000은 훈련 데이터에서 가장 많이 등장하는 상위 10,000개의 단어를 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = keras.datasets.imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플: 25000, 레이블: 25000\n"
     ]
    }
   ],
   "source": [
    "print(\"훈련 샘플: {}, 레이블: {}\".format(len(train_data), len(train_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터마다 단어의 개수가 다르다. 신경망의 입력은 길이가 같아야 하므로 해결해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218, 189)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[0]), len(train_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정수를 문자로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# 단어와 정수 인덱스를 매핑한 딕셔너리\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# 처음 몇 개 인덱스는 사전에 정의되어 있습니다\n",
    "word_index = {k:(v+3) for k,v in word_index.items()} \n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_review(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리, 다시 문자를 숫자로 바꿈, padding 적용, 문자 길이 통일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n",
    "                                                        value=word_index[\"<PAD>\"],\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=256)\n",
    "\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n",
    "                                                       value=word_index[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[0]), len(train_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1   14   22   16   43  530  973 1622 1385   65  458 4468   66 3941\n",
      "    4  173   36  256    5   25  100   43  838  112   50  670    2    9\n",
      "   35  480  284    5  150    4  172  112  167    2  336  385   39    4\n",
      "  172 4536 1111   17  546   38   13  447    4  192   50   16    6  147\n",
      " 2025   19   14   22    4 1920 4613  469    4   22   71   87   12   16\n",
      "   43  530   38   76   15   13 1247    4   22   17  515   17   12   16\n",
      "  626   18    2    5   62  386   12    8  316    8  106    5    4 2223\n",
      " 5244   16  480   66 3785   33    4  130   12   16   38  619    5   25\n",
      "  124   51   36  135   48   25 1415   33    6   22   12  215   28   77\n",
      "   52    5   14  407   16   82    2    8    4  107  117 5952   15  256\n",
      "    4    2    7 3766    5  723   36   71   43  530  476   26  400  317\n",
      "   46    7    4    2 1029   13  104   88    4  381   15  297   98   32\n",
      " 2071   56   26  141    6  194 7486   18    4  226   22   21  134  476\n",
      "   26  480    5  144   30 5535   18   51   36   28  224   92   25  104\n",
      "    4  226   65   16   38 1334   88   12   16  283    5   16 4472  113\n",
      "  103   32   15   16 5345   19  178   32    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 모델 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,289\n",
      "Trainable params: 160,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 입력 크기는 영화 리뷰 데이터셋에 적용된 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "vocab_size = 10000\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 16, input_shape=(None,)))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "층을 순서대로 쌓아 분류기(classifier)를 만듭니다:\n",
    "\n",
    "- 첫 번째 층은 Embedding 층입니다. 이 층은 정수로 인코딩된 단어를 입력 받고 각 단어 인덱스에 해당하는 임베딩 벡터를 찾습니다. 이 벡터는 모델이 훈련되면서 학습됩니다. 이 벡터는 출력 배열에 새로운 차원으로 추가됩니다. 최종 차원은 (batch, sequence, embedding)이 됩니다.\n",
    "- 그다음 GlobalAveragePooling1D 층은 sequence 차원에 대해 평균을 계산하여 각 샘플에 대해 고정된 길이의 출력 벡터를 반환합니다. 이는 길이가 다른 입력을 다루는 가장 간단한 방법입니다.\n",
    "이 고정 길이의 출력 벡터는 16개의 은닉 유닛을 가진 완전 연결(fully-connected) 층(Dense)을 거칩니다.\n",
    "- 마지막 층은 하나의 출력 노드(node)를 가진 완전 연결 층입니다. sigmoid 활성화 함수를 사용하여 0과 1 사이의 실수를 출력합니다. 이 값은 확률 또는 신뢰도를 나타냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. loss function, optimizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델이 훈련하려면 손실 함수(loss function)과 옵티마이저(optimizer)가 필요합니다. 이 예제는 이진 분류 문제이고 모델이 확률을 출력하므로(출력층의 유닛이 하나이고 sigmoid 활성화 함수를 사용합니다), binary_crossentropy 손실 함수를 사용하겠습니다.\n",
    "\n",
    "- 다른 손실 함수를 선택할 수 없는 것은 아닙니다. 예를 들어 mean_squared_error를 선택할 수 있습니다. 하지만 일반적으로 binary_crossentropy가 확률을 다루는데 적합합니다. 이 함수는 확률 분포 간의 거리를 측정합니다. 여기에서는 정답인 타깃 분포와 예측 분포 사이의 거리입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 검증 세트 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = train_data[:10000]\n",
    "partial_x_train = train_data[10000:]\n",
    "\n",
    "y_val = train_labels[:10000]\n",
    "partial_y_train = train_labels[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 모델 훈련 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "15000/15000 [==============================] - 1s 56us/sample - loss: 0.6920 - accuracy: 0.5079 - val_loss: 0.6905 - val_accuracy: 0.5100\n",
      "Epoch 2/40\n",
      "15000/15000 [==============================] - 1s 40us/sample - loss: 0.6867 - accuracy: 0.6101 - val_loss: 0.6830 - val_accuracy: 0.6251\n",
      "Epoch 3/40\n",
      "15000/15000 [==============================] - 1s 41us/sample - loss: 0.6746 - accuracy: 0.6811 - val_loss: 0.6677 - val_accuracy: 0.7113\n",
      "Epoch 4/40\n",
      "15000/15000 [==============================] - 1s 40us/sample - loss: 0.6527 - accuracy: 0.7273 - val_loss: 0.6424 - val_accuracy: 0.7582\n",
      "Epoch 5/40\n",
      "15000/15000 [==============================] - 1s 40us/sample - loss: 0.6198 - accuracy: 0.7752 - val_loss: 0.6078 - val_accuracy: 0.7819\n",
      "Epoch 6/40\n",
      "15000/15000 [==============================] - 1s 40us/sample - loss: 0.5764 - accuracy: 0.8105 - val_loss: 0.5651 - val_accuracy: 0.7966\n",
      "Epoch 7/40\n",
      "15000/15000 [==============================] - 1s 40us/sample - loss: 0.5269 - accuracy: 0.8292 - val_loss: 0.5184 - val_accuracy: 0.8227\n",
      "Epoch 8/40\n",
      "15000/15000 [==============================] - 1s 40us/sample - loss: 0.4773 - accuracy: 0.8491 - val_loss: 0.4756 - val_accuracy: 0.8346\n",
      "Epoch 9/40\n",
      "15000/15000 [==============================] - 1s 40us/sample - loss: 0.4315 - accuracy: 0.8644 - val_loss: 0.4375 - val_accuracy: 0.8460\n",
      "Epoch 10/40\n",
      "15000/15000 [==============================] - 1s 40us/sample - loss: 0.3904 - accuracy: 0.8763 - val_loss: 0.4046 - val_accuracy: 0.8527\n",
      "Epoch 11/40\n",
      "15000/15000 [==============================] - 1s 40us/sample - loss: 0.3548 - accuracy: 0.8844 - val_loss: 0.3781 - val_accuracy: 0.8617\n",
      "Epoch 12/40\n",
      "15000/15000 [==============================] - 1s 39us/sample - loss: 0.3257 - accuracy: 0.8927 - val_loss: 0.3583 - val_accuracy: 0.8648\n",
      "Epoch 13/40\n",
      "15000/15000 [==============================] - 1s 41us/sample - loss: 0.3023 - accuracy: 0.8989 - val_loss: 0.3417 - val_accuracy: 0.8707\n",
      "Epoch 14/40\n",
      "15000/15000 [==============================] - 1s 40us/sample - loss: 0.2817 - accuracy: 0.9043 - val_loss: 0.3297 - val_accuracy: 0.8745\n",
      "Epoch 15/40\n",
      "15000/15000 [==============================] - 1s 40us/sample - loss: 0.2645 - accuracy: 0.9093 - val_loss: 0.3198 - val_accuracy: 0.8766\n",
      "Epoch 16/40\n",
      "15000/15000 [==============================] - 1s 40us/sample - loss: 0.2493 - accuracy: 0.9154 - val_loss: 0.3120 - val_accuracy: 0.8762\n",
      "Epoch 17/40\n",
      "15000/15000 [==============================] - 1s 40us/sample - loss: 0.2353 - accuracy: 0.9205 - val_loss: 0.3056 - val_accuracy: 0.8793\n",
      "Epoch 18/40\n",
      "15000/15000 [==============================] - 1s 40us/sample - loss: 0.2229 - accuracy: 0.9240 - val_loss: 0.3003 - val_accuracy: 0.8821\n",
      "Epoch 19/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.2127 - accuracy: 0.92 - 1s 40us/sample - loss: 0.2117 - accuracy: 0.9259 - val_loss: 0.2959 - val_accuracy: 0.8818\n",
      "Epoch 20/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.2034 - accuracy: 0.93 - 1s 40us/sample - loss: 0.2017 - accuracy: 0.9311 - val_loss: 0.2930 - val_accuracy: 0.8825\n",
      "Epoch 21/40\n",
      "15000/15000 [==============================] - 1s 41us/sample - loss: 0.1914 - accuracy: 0.9362 - val_loss: 0.2906 - val_accuracy: 0.8825\n",
      "Epoch 22/40\n",
      "15000/15000 [==============================] - 1s 40us/sample - loss: 0.1829 - accuracy: 0.9396 - val_loss: 0.2886 - val_accuracy: 0.8840\n",
      "Epoch 23/40\n",
      "15000/15000 [==============================] - 1s 40us/sample - loss: 0.1744 - accuracy: 0.9437 - val_loss: 0.2883 - val_accuracy: 0.8831\n",
      "Epoch 24/40\n",
      "15000/15000 [==============================] - 1s 39us/sample - loss: 0.1669 - accuracy: 0.9463 - val_loss: 0.2876 - val_accuracy: 0.8843\n",
      "Epoch 25/40\n",
      "15000/15000 [==============================] - 1s 40us/sample - loss: 0.1593 - accuracy: 0.9497 - val_loss: 0.2865 - val_accuracy: 0.8859\n",
      "Epoch 26/40\n",
      "15000/15000 [==============================] - 1s 40us/sample - loss: 0.1526 - accuracy: 0.9525 - val_loss: 0.2880 - val_accuracy: 0.8832\n",
      "Epoch 27/40\n",
      "15000/15000 [==============================] - 1s 40us/sample - loss: 0.1462 - accuracy: 0.9552 - val_loss: 0.2879 - val_accuracy: 0.8845\n",
      "Epoch 28/40\n",
      "15000/15000 [==============================] - 1s 40us/sample - loss: 0.1400 - accuracy: 0.9569 - val_loss: 0.2894 - val_accuracy: 0.8843\n",
      "Epoch 29/40\n",
      "15000/15000 [==============================] - 1s 40us/sample - loss: 0.1346 - accuracy: 0.9601 - val_loss: 0.2911 - val_accuracy: 0.8843\n",
      "Epoch 30/40\n",
      "15000/15000 [==============================] - 1s 40us/sample - loss: 0.1290 - accuracy: 0.9624 - val_loss: 0.2912 - val_accuracy: 0.8860\n",
      "Epoch 31/40\n",
      "15000/15000 [==============================] - 1s 40us/sample - loss: 0.1232 - accuracy: 0.9647 - val_loss: 0.2928 - val_accuracy: 0.8862\n",
      "Epoch 32/40\n",
      "15000/15000 [==============================] - 1s 40us/sample - loss: 0.1181 - accuracy: 0.9675 - val_loss: 0.2949 - val_accuracy: 0.8847\n",
      "Epoch 33/40\n",
      "15000/15000 [==============================] - 1s 40us/sample - loss: 0.1132 - accuracy: 0.9685 - val_loss: 0.2978 - val_accuracy: 0.8838\n",
      "Epoch 34/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.1081 - accuracy: 0.97 - 1s 40us/sample - loss: 0.1088 - accuracy: 0.9701 - val_loss: 0.3008 - val_accuracy: 0.8838\n",
      "Epoch 35/40\n",
      "15000/15000 [==============================] - 1s 40us/sample - loss: 0.1046 - accuracy: 0.9707 - val_loss: 0.3037 - val_accuracy: 0.8848\n",
      "Epoch 36/40\n",
      "15000/15000 [==============================] - 1s 40us/sample - loss: 0.1003 - accuracy: 0.9731 - val_loss: 0.3059 - val_accuracy: 0.8830\n",
      "Epoch 37/40\n",
      "15000/15000 [==============================] - 1s 40us/sample - loss: 0.0961 - accuracy: 0.9745 - val_loss: 0.3093 - val_accuracy: 0.8832\n",
      "Epoch 38/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.0923 - accuracy: 0.97 - 1s 39us/sample - loss: 0.0922 - accuracy: 0.9763 - val_loss: 0.3136 - val_accuracy: 0.8820\n",
      "Epoch 39/40\n",
      "15000/15000 [==============================] - 1s 40us/sample - loss: 0.0891 - accuracy: 0.9775 - val_loss: 0.3182 - val_accuracy: 0.8816\n",
      "Epoch 40/40\n",
      "15000/15000 [==============================] - 1s 40us/sample - loss: 0.0850 - accuracy: 0.9793 - val_loss: 0.3214 - val_accuracy: 0.8813\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=40,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 모델 평가 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 0s 13us/sample - loss: 0.3431 - accuracy: 0.8703\n",
      "[0.34308244653224945, 0.87028]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_data, test_labels)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 그래프에서 점선은 훈련 손실과 훈련 정확도를 나타냅니다. 실선은 검증 손실과 검증 정확도입니다.\n",
    "\n",
    "훈련 손실은 에포크마다 감소하고 훈련 정확도는 증가한다는 것을 주목하세요. 경사 하강법 최적화를 사용할 때 볼 수 있는 현상입니다. 매 반복마다 최적화 대상의 값을 최소화합니다.\n",
    "\n",
    "하지만 검증 손실과 검증 정확도에서는 그렇지 못합니다. 약 20번째 에포크 이후가 최적점인 것 같습니다. 이는 과대적합 때문입니다. 이전에 본 적 없는 데이터보다 훈련 데이터에서 더 잘 동작합니다. 이 지점부터는 모델이 과도하게 최적화되어 테스트 데이터에서 일반화되기 어려운 훈련 데이터의 특정 표현을 학습합니다.\n",
    "\n",
    "여기에서는 과대적합을 막기 위해 단순히 20번째 에포크 근처에서 훈련을 멈출 수 있습니다. 콜백(callback)을 사용하여 자동으로 이렇게 하는 방법이 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
